# -*- coding: utf-8 -*-
"""EQ2425_cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qGUaRcfD0jZrO4w1TGIA5xdYcszUO24n

Code for lab 3 of EQ2425 Analysis and Search of Visual Data by Angelo Delli Santi and Mar Balibrea Rull.

# **INIT**
"""

!pip install -Uqq fastbook

import fastbook

fastbook.setup_book()

from fastbook import *

import torch
import torchvision
import torchvision.transforms as transforms

import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import time
import numpy as np
import os
import glob

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

os.listdir('gdrive/My Drive/uni/EQ2425')

# https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html?fbclid=IwAR1hIVWsjxv6OX_tiaWdqQ9jsZEbQQ_7Llyl4gWaqqfK_og8jopnzWE-1K8

class Net(nn.Module):

    def __init__(self):

        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 24, 5, 1, 2)
        self.pool1 = nn.MaxPool2d(2, 2)

        self.conv2 = nn.Conv2d(24, 48, 3, 1, 1)
        self.pool2 = nn.MaxPool2d(2, 2)

        self.conv3 = nn.Conv2d(48, 96, 3, 1, 1)
        self.pool3 = nn.MaxPool2d(2, 2)

        self.fc1 = nn.Linear(96 * 4 * 4, 512)
        self.fc2 = nn.Linear(512, 10)


    def forward(self, x):

        x = self.pool1(F.relu(self.conv1(x)))
        x = self.pool2(F.relu(self.conv2(x)))
        x = self.pool3((self.conv3(x)))
        x = x.view(-1, 96 * 4 * 4)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

class NetA1(nn.Module):

    def __init__(self):

        super(NetA1, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, 5, 1, 2)
        self.pool1 = nn.MaxPool2d(2, 2)

        self.conv2 = nn.Conv2d(64, 128, 3, 1, 1)
        self.pool2 = nn.MaxPool2d(2, 2)

        self.conv3 = nn.Conv2d(128, 256, 3, 1, 1)
        self.pool3 = nn.MaxPool2d(2, 2)

        self.fc1 = nn.Linear(256 * 4 * 4, 512)
        self.fc2 = nn.Linear(512, 10)


    def forward(self, x):

        x = self.pool1(F.relu(self.conv1(x)))
        x = self.pool2(F.relu(self.conv2(x)))
        x = self.pool3((self.conv3(x)))
        x = x.view(-1, 256 * 4 * 4)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

class NetA1point5(nn.Module):

    def __init__(self):

        super(NetA1point5, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 5, 1, 2)
        self.pool1 = nn.MaxPool2d(2, 2)

        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)
        self.pool2 = nn.MaxPool2d(2, 2)

        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)
        self.pool3 = nn.MaxPool2d(2, 2)

        self.fc1 = nn.Linear(128 * 4 * 4, 512)
        self.fc2 = nn.Linear(512, 10)


    def forward(self, x):

        x = self.pool1(F.relu(self.conv1(x)))
        x = self.pool2(F.relu(self.conv2(x)))
        x = self.pool3((self.conv3(x)))
        x = x.view(-1, 128 * 4 * 4)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

class NetA2(nn.Module):

    def __init__(self):

        super(NetA2, self).__init__()
        self.conv1 = nn.Conv2d(3, 24, 5, 1, 2)
        self.pool1 = nn.MaxPool2d(2, 2)

        self.conv2 = nn.Conv2d(24, 48, 3, 1, 1)
        self.pool2 = nn.MaxPool2d(2, 2)
        
        self.conv3 = nn.Conv2d(48, 96, 3, 1, 1)
        self.pool3 = nn.MaxPool2d(2, 2)

        self.fc1 = nn.Linear(96 * 4 * 4, 512)
        self.fc2 = nn.Linear(512, 128)
        self.fc3 = nn.Linear(128, 10)


    def forward(self, x):

        x = self.pool1(F.relu(self.conv1(x)))
        x = self.pool2(F.relu(self.conv2(x)))
        x = self.pool3((self.conv3(x)))
        x = x.view(-1, 96 * 4 * 4)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

class NetB(nn.Module): # based on A1, as that was the best performing in A

    def __init__(self):

        super(NetB, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, 7, 1, 3)
        self.pool1 = nn.MaxPool2d(2, 2)

        self.conv2 = nn.Conv2d(64, 128, 5, 1, 2)
        self.pool2 = nn.MaxPool2d(2, 2)

        self.conv3 = nn.Conv2d(128, 256, 3, 1, 1)
        self.pool3 = nn.MaxPool2d(2, 2)

        self.fc1 = nn.Linear(256 * 4 * 4, 512)
        self.fc2 = nn.Linear(512, 10)


    def forward(self, x):

        x = self.pool1(F.relu(self.conv1(x)))
        x = self.pool2(F.relu(self.conv2(x)))
        x = self.pool3((self.conv3(x)))
        x = x.view(-1, 256 * 4 * 4)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

class NetC(nn.Module): # based on A1+B, as it performed better than A1
  
    def __init__(self):
      
        super(NetC, self).__init__()
        self.neg = 0
        self.pos = 0
        self.conv1 = nn.Conv2d(3, 64, 7, 1, 3)
        self.pool1 = nn.MaxPool2d(2, 2)

        self.conv2 = nn.Conv2d(64, 128, 5, 1, 2)
        self.pool2 = nn.MaxPool2d(2, 2)

        self.conv3 = nn.Conv2d(128, 256, 3, 1, 1)
        self.pool3 = nn.MaxPool2d(2, 2)

        self.fc1 = nn.Linear(256 * 4 * 4, 512)
        self.fc2 = nn.Linear(512, 10)

        self.lrelu = nn.LeakyReLU()


    def forward(self, x):
        x = self.conv1(x)
        x = self.lrelu(x)
        x = self.pool1(x)
        x = self.pool2(self.lrelu(self.conv2(x)))
        x = self.pool3((self.conv3(x)))
        x = x.view(-1, 256 * 4 * 4)
        x = self.fc1(x)
        self.neg += np.sum(x.cpu().detach().numpy() < 0)
        self.pos += np.sum(x.cpu().detach().numpy() >= 0)
        x = self.lrelu(x)
        x = self.fc2(x)
        return x

class NetD(nn.Module): # based on A1+B, as it performed equal than A1+B+C

    def __init__(self):

        super(NetD, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, 7, 1, 3)
        self.pool1 = nn.MaxPool2d(2, 2)

        self.conv2 = nn.Conv2d(64, 128, 5, 1, 2)
        self.pool2 = nn.MaxPool2d(2, 2)

        self.conv3 = nn.Conv2d(128, 256, 3, 1, 1)
        self.pool3 = nn.MaxPool2d(2, 2)

        self.fc1 = nn.Linear(256 * 4 * 4, 512)
        self.dropout = nn.Dropout(0.3)
        self.fc2 = nn.Linear(512, 10)


    def forward(self, x):

        x = self.pool1(F.relu(self.conv1(x)))
        x = self.pool2(F.relu(self.conv2(x)))
        x = self.pool3((self.conv3(x)))
        x = x.view(-1, 256 * 4 * 4)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

class NetE(nn.Module): # based on A1+B+D, as it performed better than A1+B

    def __init__(self):

        super(NetE, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, 7, 1, 3)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.norm1 = nn.BatchNorm2d(64)

        self.conv2 = nn.Conv2d(64, 128, 5, 1, 2)
        self.pool2 = nn.MaxPool2d(2, 2)
        self.norm2 = nn.BatchNorm2d(128)

        self.conv3 = nn.Conv2d(128, 256, 3, 1, 1)
        self.pool3 = nn.MaxPool2d(2, 2)

        self.fc1 = nn.Linear(256 * 4 * 4, 512)
        self.norm3 = nn.BatchNorm1d(512)
        self.dropout = nn.Dropout(0.3)
        self.fc2 = nn.Linear(512, 10)


    def forward(self, x):

        x = self.pool1(self.norm1(F.relu(self.conv1(x))))
        x = self.pool2(self.norm2(F.relu(self.conv2(x))))
        x = self.pool3((self.conv3(x)))
        x = x.view(-1, 256 * 4 * 4)
        x = self.norm3(F.relu(self.fc1(x)))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

PATH = 'gdrive/My Drive/uni/EQ2425'
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

"""# **TRAINING**"""

def train(net, train_it, name, lr):

  net.to(device)

  criterion = nn.CrossEntropyLoss()
  optimizer = optim.SGD(net.parameters(), lr = lr, momentum = 0.9)

  t1 = time.time()
  early_stopping = False
  epoch_loss = []
  epochs = 300
  size = 0
  for epoch in range(epochs):  # loop over the dataset multiple times

      running_loss = 0.0
      for i, data in enumerate(train_it, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data[0].to(device), data[1].to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
      epoch_loss.append(running_loss / (i + 1))
      print('[%d] loss: %.4f (%d s)' % (epoch + 1, epoch_loss[-1], int(time.time() - t1)))

      if epoch > 100 and np.mean(np.abs(np.diff(epoch_loss[-5:]))) < 1e-5:
        early_stopping = True

      if epoch % 10 == 0 or epoch == epochs-1 or early_stopping:
        torch.save(net.state_dict(), PATH + '/models/' + name + '.pth')
        with open(PATH + '/models/' + name + '_error.npy', 'wb') as f:
          np.save(f, np.array(epoch_loss))
        if os.stat(PATH + '/models/' + name + '_error.npy').st_size > size:
          size = os.stat(PATH + '/models/' + config_name + '_error.npy').st_size
        else:
          print('file is not updating!!!')
      
      if early_stopping:
        break

  print('Finished training in {:.0f} seconds'.format(time.time() - t1))

# before every training configuration, remember to change the net!!!

batch_size = 64
learning_rate = 0.001
shuffleing = False
config_name = 'test_lrelu'
net = NetC()

transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (1, 1, 1))])

trainset = torchvision.datasets.CIFAR10(root = PATH + '/data', train = True, download = True, transform = transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle = shuffleing, num_workers = 2)

train(net = net, train_it = trainloader, name = config_name, lr = learning_rate)

with open(PATH + '/models/' + config_name + '_error.npy', 'rb') as f:
  a = np.load(f)
a.shape

"""# **TESTING**"""

## test

testset = torchvision.datasets.CIFAR10(root = PATH + '/data', train = False, download = True, transform = transform)
testloader = torch.utils.data.DataLoader(testset, batch_size = 10000, num_workers = 2)

dataiter = iter(testloader)
images, labels = dataiter.next()
images, labels = images.to(device), labels.to(device)

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

# before every testing, remember to change the net type and the config_name of the model!
net = NetA1point5()
net.to(device)
net.load_state_dict(torch.load(PATH + '/models/' + config_name + '.pth'))

class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))
with torch.no_grad():
  outputs = net(images)
  _, predicted = torch.max(outputs, 1)
  c = (predicted == labels).squeeze()
  for i in range(10000):
      label = labels[i]
      class_correct[label] += c[i].item()
      class_total[label] += 1

tot_acc = 100 * np.sum(class_correct) / np.sum(class_total)
print('Accuracy of the network on the 10000 test images: %d %%' % tot_acc)
for i in range(10):
    print('Accuracy of %5s : %2d%%' % (classes[i], 100 * class_correct[i]/ class_total[i]))

cf = confusion_matrix(labels.cpu().detach(), predicted.cpu().detach())

with open(PATH + '/models/' + config_name + '_cf.npy','wb') as f:
    np.save(f, cf)

disp = ConfusionMatrixDisplay(confusion_matrix = cf, display_labels = classes)

disp = disp.plot(xticks_rotation = 'vertical', values_format = 'd')
plt.title('Confusion matrix for ' + config_name + ' (acc %2d%%)' % (tot_acc))

plt.savefig(PATH + '/models/' + config_name + '_cf.png')

a = sorted(glob.glob(PATH + '/models/*_it2_error.npy'))
legend = []
for i in a:
  legend.append(i.split('/')[-1].split('_')[0])
  error = np.load(i)
  plt.plot(error)
plt.legend(legend)
plt.xlabel('Epoch')
plt.ylabel('Negative Log Likelihood Loss')
plt.savefig(PATH + '/network_stucture_loss.png')

PATH + '/network_stucture_loss.png'